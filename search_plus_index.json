{"./":{"url":"./","title":"Description","keywords":"","body":"\"Kubernetes Hardening Guide\" Kubernetes Hardening Guidance (View original English PDF) is provided by Released by the U.S. National Security Agency (NSA) in August 2021, see publication information for details, if you find errors, you are welcome to submit corrections (known [Corrigendum.md]) on GitHub. License You can share it using the Attribution-NonCommercial-ShareAlike 4.0 (CC BY-NC-SA 4.0) license . Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"notices-and-hitory.html":{"url":"notices-and-hitory.html","title":"Notices and History","keywords":"","body":"Notifications and History File Change History English version Date Version Description August 2021 1.0 First Release WARRANTY AND ENDORSEMENT DISCLAIMER The information and opinions in this document are provided \"as is\" without any warranty or guarantee. Reference in this document to any specific commercial product, program, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the U.S. Government, and this guide may not be used in advertising or The purpose of product endorsement. Trademark Approval Kubernetes is a registered trademark of the Linux Foundation. SELinux is a registered trademark of the U.S. National Security Agency. AppArmor is a registered trademark of SUSE LLC. Windows and Hyper-V are registered trademarks of Microsoft Corporation. ETCD is a registered trademark of CoreOS, Inc. Syslog-ng is a registered trademark of One Identity Software International Designated Activity, Inc. Prometheus is a registered trademark of the Linux Foundation. Grafana is a registered trademark of Raintank, Inc.dba Grafana Labs. Elasticsearch and ELK Stack are registered trademarks of Elasticsearch B.V. Copyright Confirmation The information, examples, and numbers in this document are based on the Kubernetes authors' Kubernetes Documentation, published under Creative Commons Attribution 4.0 License Released. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:47 "},"publication-information.html":{"url":"publication-information.html","title":"publication information","keywords":"","body":"Publication information author Cybersecurity and Infrastructure Security Agency (CISA) National Security Agency (NSA) Cybersecurity Directorate Endpoint Security contact information Customer Requests / General Cybersecurity Issues. Cybersecurity Requests Center, 410-854-4200, Cybersecurity_Requests@nsa.gov. Media Inquiry / News Desk Media Relations, 443-634-0721, MediaRelations@nsa.gov. For incident response resources, please contact CISA: CISAServiceDesk@cisa.dhs.gov. purpose The National Security Agency and CISA have developed this document to promote their respective cybersecurity efforts, including their responsibilities for developing and disseminating cybersecurity specifications and mitigation measures. This information can be shared widely to reach all appropriate stakeholders. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:47 "},"executive-summary.html":{"url":"executive-summary.html","title":"executive summary","keywords":"","body":"executive Summary Kubernetes® is an open source system that automates the deployment, scaling, and management of applications running in containers and often hosted in cloud environments. Using this type of virtualized infrastructure can provide some flexibility and security benefits compared to traditional monolithic software platforms. However, securely managing everything from microservices to the underlying infrastructure introduces additional complexities. The hardening guidance detailed in this report is designed to help organizations deal with the associated risks and reap the benefits of using this technology. Three common sources of breaches in Kubernetes are supply chain risks, malicious actors, and insider threats. Supply chain risks are often challenging and can arise during the container build cycle or infrastructure acquisition. Malicious threat actors can exploit vulnerabilities and misconfigurations in components of the Kubernetes architecture, such as the control plane, worker nodes, or containerized applications. Insider threats can be administrators, users, or cloud service providers. Insiders with special access to an organization's Kubernetes infrastructure may abuse these privileges. This guide describes the security challenges associated with setting up and securing a Kubernetes cluster. Includes hardening strategies to avoid common misconfigurations and guides system administrators and developers of national security systems on how to deploy Kubernetes, with configuration examples of recommended hardening and mitigation measures. This guide details the following mitigation measures: Scan containers and pods for vulnerabilities or misconfigurations. Run containers and pods with as few permissions as possible. Use network isolation to control the amount of damage a vulnerability can cause. Use firewalls to restrict unwanted network connections and encryption to protect confidentiality. Use strong authentication and authorization to restrict user and administrator access and limit the attack surface. Use log auditing so administrators can monitor activity and warn of potentially malicious activity. Regularly review all Kubernetes setups and use vulnerability scanning to help ensure risks are properly considered and security patches applied. For additional security hardening guidance, see the Center for Internet Security Kubernetes Benchmarks, Docker and Kubernetes Security Technology Implementation Guide, Cybersecurity and Infrastructure Security Agency (CISA) Analytical Reports, and Kubernetes documentation. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"corrigendum.html":{"url":"corrigendum.html","title":"Corrigendum","keywords":"","body":"Errata Correction 1 PDF original page 4, kubelet port, the default should be 10250, not 10251. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"introduction.html":{"url":"introduction.html","title":"Introduction","keywords":"","body":"Introduction Kubernetes, often abbreviated as \"K8s\", is an open source container or orchestration system for automating the deployment, scaling and management of containerized applications. It manages all the elements that make up a cluster, from each microservice in the application to the entire cluster. Using containerized applications as microservices can provide more flexibility and security benefits than a monolithic software platform, but it can also introduce other complications. Figure 1: High-level view of Kubernetes cluster components This guide focuses on security challenges and, where possible, proposes hardening strategies for administrators of national security systems and critical infrastructure. Although this guidance is targeted at national security systems and critical infrastructure organizations, administrators of federal and state, local, tribal, and territorial (SLTT) government networks are also encouraged to implement the recommendations provided. Security issues with Kubernetes clusters can be complex and are often abused in potential threats that exploit their misconfiguration. The following guides provide specific security configurations to help build more secure Kubernetes clusters. suggestion A summary of the main recommendations for each section is as follows: Kubernetes Pod security Use the built container to run the application as a non-root user Where possible, run containers with immutable file systems Scan container images for possible vulnerabilities or misconfigurations Use Pod Security Policies to enforce a minimum level of security, including: Prevent privileged containers Deny container functions that are often exploited to break through, such as hostPID, hostIPC, hostNetwork, allowedHostPath, etc. Deny containers that execute as root or allow elevation to root Use security services such as SELinux®, AppArmor®, and seccomp to harden applications and prevent exploitation. Network isolation and hardening Use firewalls and role-based access control (RBAC) to lock down access to control plane nodes Further restrict access to Kubernetes etcd server Configure control plane components to use Transport Layer Security (TLS) certificates for authentication and encrypted communication Set network policies to isolate resources. Pods and services in different namespaces can still communicate with each other unless additional isolation is implemented, such as network policies Place all credentials and sensitive information in Kubernetes Secrets, not configuration files. Encrypt Secrets using strong encryption methods Authentication and authorization Disable anonymous login (enabled by default) Use strong user authentication Create RBAC policies to restrict administrator, user and service account activity Log audit Enable audit logging (default is disabled) Continuously save logs to ensure availability in the event of node, Pod or container level failure Configure a metric logger Upgrade and apply security practices Immediately apply security patches and updates Conduct regular vulnerability scanning and penetration testing Remove components from the environment when they are no longer needed Architecture Overview Kubernetes uses a cluster architecture. A Kubernetes cluster is composed of some control planes and one or more physical or virtual machines, called worker nodes. Worker nodes host Pods, which contain one or more containers. A container is an executable image that contains a software package and all its dependencies. See Figure 2: Kubernetes architecture. Figure 2: Kubernetes architecture The control plane makes decisions about the cluster. This includes scheduling the running of containers, detecting/reacting to failures, and starting new Pods if the number of replicas specified in the deployment file is not met. The following logical components are part of the control plane: Controller manager (default port: 10252) - Monitors Kubernetes clusters to detect and maintain several aspects of the Kubernetes environment, including adding Pods to services, maintaining the correct number of Pods in a set, and responding to the loss of nodes React. Cloud controller manager (default port: 10258) - An optional component for cloud-based deployments. The cloud controller interfaces with the cloud service provider to manage the cluster's load balancer and virtual network. Kubernetes API Server (default port: 6443 or 8080) - The interface for administrators to operate Kubernetes. Therefore, API servers are often exposed outside the control plane. The API server is designed to be scalable and may exist on multiple control plane nodes. Etcd (default port range: 2379-2380) - A persistent backup store where all information about the cluster status is saved. Etcd should not be manipulated directly, but should be managed through the API server. Scheduler (default port: 10251) - Tracks the status of worker nodes and decides where to run pods. Kube-scheduler can only be accessed by nodes within the control plane. Kubernetes worker nodes are physical or virtual machines dedicated to running containerized applications for the cluster. In addition to running the container engine, worker nodes host the following two services, allowing coordination from the control plane: Kubelet (default port: 10250) - Runs on each worker node to coordinate and verify the execution of Pods. Kube-proxy - A network proxy that uses the host's packet filtering capabilities to ensure correct routing of packets within a Kubernetes cluster. Clusters are typically hosted using a cloud service provider's (CSP) Kubernetes service or hosted on-premises. When designing a Kubernetes environment, organizations should understand their responsibilities in maintaining the cluster securely. The CSP manages most Kubernetes services, but organizations may need to handle certain aspects such as authentication and authorization. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"threat-model.html":{"url":"threat-model.html","title":"Threat Modeling","keywords":"","body":"Threat modeling Kubernetes can be an important target for data and/or computing power theft. While data theft has traditionally been the primary motivation, cyber actors seeking computing power (often for cryptocurrency mining) are also drawn to Kubernetes to exploit its underlying infrastructure. In addition to resource theft, cyber actors can also cause denial of service against Kubernetes. The threats below represent the most likely sources of disruption to Kubernetes clusters. Supply Chain Risk - Attack vectors against supply chains are diverse and challenging to mitigate risks. Supply chain risk is the risk that an adversary could subvert any element that makes up a system, including product components, services, or people that help deliver the final product. This may include third-party software and vendors used to create and manage Kubernetes clusters. Potential threats to the supply chain can impact Kubernetes on multiple levels, including: Container/Application Level - The security of applications running in Kubernetes and their third-party dependencies relies on the trustworthiness of developers and the defense capabilities of the development infrastructure. Malicious containers or applications from third parties can provide cyber actors a foothold in the cluster. Infrastructure - The underlying systems hosting Kubernetes have their own software and hardware dependencies. Any potential threat to a system that is part of a worker node or control plane could provide a cyber actor with a foothold within the cluster. Malicious Threat Actors - Malicious actors often exploit vulnerabilities to gain access from remote locations. The Kubernetes architecture exposes several APIs that could potentially be exploited by cyber actors for remote exploitation. Control Plane - The Kubernetes control plane has various components that communicate to track and manage the cluster. Cyber actors often exploit exposed control plane components that lack proper access controls. Worker Node - In addition to running the container engine, worker nodes also host the kubelet and kube-proxy services, which may be exploited by network actors. Additionally, worker nodes exist outside of a locked-down control plane and may be more easily exploited by network actors. Containerized Applications - Applications running within a cluster are common targets. Applications are often accessible outside the cluster, making them accessible to remote network actors. Cyber actors can then work from an already compromised application or leverage the exposed application's internally accessible resources to escalate privileges within the cluster. Insider Threats - Threat actors can exploit vulnerabilities or use privileges gained by individuals while working within an organization. Individuals from within the organization are given special knowledge and privileges that can be used to compromise Kubernetes clusters. Administrator - Kubernetes administrators have control over running containers, including the ability to execute arbitrary commands within a containerized environment. Kubernetes-enforced RBAC authorization can help reduce risk by limiting access to sensitive capabilities. However, since Kubernetes lacks two-person integrity control, you must have at least one administrative account to gain control of the cluster. Administrators often have physical access to systems or hypervisors, which can also be used to compromise a Kubernetes environment. User - A user of a containerized application who may have the knowledge and credentials to access containerized services in a Kubernetes cluster. This level of access can provide sufficient means to leverage the application itself or other cluster components. Cloud service or infrastructure provider - Access to the physical systems or hypervisors that manage Kubernetes nodes can be used to compromise the Kubernetes environment. Cloud service providers often have multiple layers of technical and administrative controls to protect systems from privileged administrators. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:47 "},"kubernetes-pod-security.html":{"url":"kubernetes-pod-security.html","title":"Kubernetes Pod Security","keywords":"","body":"Kubernetes Pod Security Pod is the smallest deployable unit in Kubernetes, consisting of one or more containers. Pods are typically the initial execution environment for network actors when leveraging containers. For this reason, Pods should be hardened to make exploitation more difficult and limit the impact of a successful compromise. Figure 3: Pod component with sidecar proxy as log container \"Non-root\" containers and \"no-root\" container engines By default, many container services run as the privileged root user, and applications execute as root within the container, although privileged execution is not required. You can limit the impact of a compromised container by using non-root containers or a rootless container engine to prevent root execution. Both methods can have a significant impact on the runtime environment, so applications should be fully tested to ensure compatibility. Non-root containers: Container Engine allows containers to run applications as non-root users and non-root group members. Typically, this non-default setting is configured when building the container image. Appendix A: Example Dockerfile for non-root applications shows an example Dockerfile that runs an application as a non-root user. Non-root user. Additionally, Kubernetes can load containers into Pods when SecurityContext:runAsUser specifies a non-zero user. While the runAsUser directive effectively forces non-root execution at deployment time, the NSA and CISA encourage developers to build container applications that execute as a non-root user. Integrating non-root execution at build time provides better assurance that your application will function properly without root privileges. Rootless Container Engines: Some container engines can run in an unprivileged context instead of using a daemon that runs as root. In this case, from the perspective of the containerized application, execution appears to be using the root user, but execution is remapped to the engine user context on the host machine. While rootless container engines add an effective layer of security, many engines are currently released as experimental and should not be used in production environments. Administrators should be aware of this emerging technology and look to adopt rootless container engines when vendors release stable versions that are compatible with Kubernetes. Immutable container file system By default, containers are allowed to execute without restrictions in their own context. A cyber actor who gains execution permissions within a container can create files, download scripts, and modify applications within the container. Kubernetes can lock down a container's file system, preventing many post-exposure activities. However, these limitations also affect legitimate container applications and can cause crashes or unexpected behavior. To prevent compromise of legitimate applications, Kubernetes administrators can mount a secondary read/write file system for specific directories that applications require write access to. Appendix B: Deployment template example for a read-only file system shows an example of an immutable container with a writable directory. Build secure container images Container images are typically created by building a container from scratch or by building on an existing image pulled from a repository. In addition to using trusted repositories to build containers, image scanning is key to ensuring the security of deployed containers. Throughout the container build workflow, images should be scanned to identify outdated libraries, known vulnerabilities, or misconfigurations such as insecure ports or permissions. Figure 4: Container build workflow, optimized with webhook and admission controller One way to implement image scanning is to use an admission controller. Admission controllers are a native feature of Kubernetes that intercept and handle requests to the Kubernetes API before the object is persisted, but after the request has been authenticated and authorized. A custom or proprietary webhook can be implemented to perform a scan before any image is deployed in the cluster. This admission controller can prevent deployment if the image complies with the organization's security policy defined in the webhook configuration. Pod Security Policy Pod creation should comply with the principle of least authorization. The Pod Security Policy (PSP) 1 is a cluster-wide policy that specifies the security requirements/defaults for Pods to execute within the cluster. While security mechanisms are typically specified in the Pod/Deployment configuration, PSP establishes a minimum security threshold that all Pods must adhere to. Some PSP fields provide default values that are used when a Pod's configuration omits a field. Other PSP fields are used to deny the creation of Pods that do not meet the requirements. PSP is performed through the Kubernetes admission controller, so PSP can only perform requirements during Pod creation. PSP does not affect Pods already running in the cluster. PSP is useful for enforcing security measures in a cluster. PSP is particularly effective for clusters managed by administrators with hierarchical roles. In these cases, top-level administrators can impose defaults and enforce requirements on lower-level administrators. The NSA and CISA encourage enterprises to adapt the Kubernetes hardened PSP template in Appendix C: Example Pod Security Policy to their needs. The following table describes some of the widely available PSP components. 表 1: Pod 安全策略组件 字段名称 使用方法 建议 privileged 控制 Pod 是否可以运行有特权的容器。 设置为 false。 hostPID、hostIPC 控制容器是否可以共享主机进程命名空间。 设置为 false。 hostNetwork 控制容器是否可以使用主机网络。 设置为 false。 allowedHostPaths 将容器限制在主机文件系统的特定路径上。 使用一个 \"假的\" 路径名称（比如 /foo 标记为只读）。省略这个字段的结果是不对容器进行准入限制。 readOnlyRootFilesystem 需要使用一个只读的根文件系统。 可能时设置为 true。 runAsUser, runAsGroup, supplementalGroups, fsGroup 控制容器应用程序是否能以 root 权限或 root 组成员身份运行。 - 设置 runAsUser 为 MustRunAsNonRoot。- 将 runAsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。 将 supplementalGroups 设置为非零（见附录 C 的例子）。将 fsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。 allowPrivilegeEscalation 限制升级到 root 权限。 设置为 false。为了有效地执行 runAsUser: MustRunAsNonRoot 设置，需要采取这一措施。 seLinux 设置容器的 SELinux 上下文。 如果环境支持 SELinux，可以考虑添加 SELinux 标签以进一步加固容器。 AppArmor 注解 设置容器所使用的 AppArmor 配置文件。 在可能的情况下，通过采用 AppArmor 来限制开发，以加固容器化的应用程序。 seccomp 注解 设置用于沙盒容器的 seccomp 配置文件。 在可能的情况下，使用 seccomp 审计配置文件来识别运行中的应用程序所需的系统调用；然后启用 seccomp 配置文件来阻止所有其他系统调用。 注意：由于以下原因，PSP 不会自动适用于整个集群： 首先，在应用 PSP 之前，必须为 Kubernetes 准入控制器启用 PodSecurityPolicy 插件，这是 kube-apiserver 的一部分。 第二，策略必须通过 RBAC 授权。管理员应从其集群组织内的每个角色中验证已实施的 PSP 的正确功能。 在有多个 PSP 的环境中，管理员应该谨慎行事，因为 Pod 的创建会遵守最小限制性授权策略。以下命令描述了给定命名空间的所有 Pod 安全策略，这可以帮助识别有问题的重叠策略。 kubectl get psp -n Protect Pod Service Account Token By default, Kubernetes automatically provides a service account (Service Account) when creating a Pod, and mounts the account's secret token (token) in the Pod at runtime. Many containerized applications do not require direct access to service accounts because Kubernetes orchestration happens transparently in the background. If an application is broken. Account tokens in Pods can be collected by network actors and used to further compromise the cluster. When an application does not require direct access to a service account, Kubernetes administrators should ensure that the Pod specification disables loading secret tokens. This can be done via the automountServiceAccountToken: false directive in the Pod's YAML specification. Harden container engine Some platforms and container engines provide additional options to harden containerized environments. A powerful example is using a hypervisor to provide container isolation. The hypervisor relies on hardware to enforce virtualization boundaries, not the operating system. Hypervisor isolation is more secure than traditional container isolation. Container engines running on Windows® operating systems can be configured to use the built-in Windows hypervisor Hyper-V® for enhanced security. Additionally, some security-focused container engines deploy each container within a lightweight hypervisor to achieve defense in depth. Hypervisor-backed containers can reduce container breaches. 1. Translator's Note: Pod Security Policy has been announced to be deprecated in version 1.21. As a replacement, 1.22 introduced the built-in Pod Security Admission controller and the new Pod Security Standards standard. Source ↩ Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"network-separation-and-hardening.html":{"url":"network-separation-and-hardening.html","title":"Network isolation and hardening","keywords":"","body":"Network isolation and reinforcement Cluster networking is a core concept of Kubernetes. Communication between containers, Pods, services and external services must be taken into account. By default, there are few network policies to isolate resources and prevent lateral movement or upgrades if the cluster is compromised. Resource isolation and encryption are effective ways to limit the movement and escalation of network actors within a cluster. key point Use network policies and firewalls to isolate resources. Securing the control plane. Encrypt traffic and sensitive data (such as Secrets) at rest. Namespaces Kubernetes namespaces are a way to divide cluster resources among multiple individuals, teams, or applications within the same cluster. By default, namespaces are not automatically isolated. However, the namespace does assign a label to a scope, which can be used to specify authorization rules through RBAC and network policies. In addition to network isolation, policies can limit storage and computing resources to provide greater control over Pods at the namespace level. There are three namespaces by default, which cannot be deleted: kube-system (for Kubernetes components) kube-public (for public resources) default (for user resources) User Pods should not be placed in kube-system or kube-public as these are reserved for cluster services. YAML files, such as Appendix D: Namespace Example, can be used to create new namespaces. Pods and services in different namespaces can still communicate with each other unless additional isolation measures are in place, such as network policies. Network Policy Network policies control traffic between pods, namespaces, and external IP addresses. By default, no network policies are applied to Pods or namespaces, resulting in unrestricted ingress and egress traffic within the Pod network. Pods are quarantined through the network policy that applies to the Pod or Pod namespace. Once a Pod is selected in a network policy, it will deny any connections that are not allowed by any applicable policy object. To create a network policy, you need a network plugin that supports the NetworkPolicy API. Use the podSelector and/or namespaceSelector options to select Pods. An example network policy is shown in Appendix E. The format of the network policy may vary depending on the Container Network Interface (CNI) plugin used by the cluster. Administrators should use the default policy of selecting all Pods to deny all ingress and egress traffic and ensure that any unselected Pods are quarantined. Additional policies can then relax these restrictions on allowed connections. External IP addresses can be used in ingress and egress policies using ipBlock, but different CNI plugins, cloud providers or service implementations may affect the order in which NetworkPolicy is processed and the rewriting of addresses within the cluster. Resource Policy In addition to network policies, LimitRange and ResourceQuota are two policies that can limit the resource usage of a namespace or node. The LimitRange policy limits individual resources per Pod or container within a specific namespace, for example, by enforcing maximum compute and storage resources. Only one LimitRange constraint can be created per namespace, as shown in the LimitRange example in Appendix F. Kubernetes 1.10 and newer support LimitRange by default. Unlike the LimitRange policy, ResourceQuotas is a limit on the total resource usage of the entire namespace, such as a limit on the total amount of CPU and memory usage. If the user attempts to create a Pod that violates the LimitRange or ResourceQuota policy, the Pod creation fails. An example of a ResourceQuota strategy is shown in Appendix G. Control plane reinforcement The control plane is the core of Kubernetes, enabling users to view containers, schedule new Pods, read Secrets, and execute commands in the cluster. Because of these sensitive functions, control planes should be highly protected. In addition to security configurations such as TLS encryption, RBAC, and strong authentication methods, network isolation can help prevent unauthorized users from accessing the control plane. The Kubernetes API server runs on ports 6443 and 8080, which should be protected by a firewall and only accept intended traffic. Port 8080, by default, is accessible from the local machine without TLS encryption, requesting bypassing the authentication and authorization modules. Insecure ports can be disabled using the API server flag --insecure-port=0. Kubernetes API servers should not be exposed to the internet or untrusted networks. Network policies can be applied to the kube-system namespace to restrict internet access to kube-system. If the default deny policy is enforced for all namespaces, the kube-system namespace must still be able to communicate with other control plane and worker nodes. The following table lists the control plane ports and services. Table 2: Control Plane Ports port direction port range destination TCP Inbound 6443 or 8080 if not disabled Kubernetes API server TCP Inbound 2379-2380 etcd server client API TCP Inbound 10250 kubelet API TCP Inbound 10251 kube-scheduler TCP Inbound 10252 kube-controller-manager TCP Inbound 10258 cloud-controller-manager（可选） Etcd The etcd backend database is a critical control plane component and the most important security part of the cluster. The etcd backend database stores state information and cluster secrets. It is a critical control plane component, and gaining write access to etcd can allow a network actor to gain root access to the entire cluster. Etcd can only be accessed through the API server, and users can be restricted by the cluster's authentication method and RBAC policy. The etcd data store can run on a separate control plane node, allowing firewalls to restrict access to the API server. Administrators should set up TLS certificates to enforce HTTPS communication between etcd server and API server. The etcd server should be configured to only trust the certificate assigned to the API server. Kubeconfig file kubeconfig files contain sensitive information about the cluster, users, namespaces and authentication mechanisms. Kubectl uses configuration files stored in the $HOME/.kube directory of worker nodes and controls the plane local machine. A network actor could exploit access to this configuration directory to obtain and modify configurations or credentials, further compromising the cluster. Configuration files should be protected against unintentional changes, and unauthenticated non-root users should be blocked from accessing these files. Work node division The worker node can be a virtual machine or a physical machine, depending on the cluster implementation. Because nodes run microservices and host the cluster's network applications, they are often targets of attacks. If a node is compromised, administrators should proactively limit the attack surface by separating worker nodes from other network segments that do not need to communicate with worker nodes or Kubernetes services. Firewalls can be used to separate internal network segments from external-facing worker nodes or the entire Kubernetes service, depending on the network. Confidential databases or internal services that do not require internet access, which may need to be separated from the possible attack surface of worker nodes. The following table lists the worker node ports and services. Table 3: Worker node ports port direction port range destination TCP Inbound 10250 kubelet API TCP Inbound 30000-32767 NodePort Services Encryption Administrators should configure all traffic in a Kubernetes cluster—including traffic between components, nodes, and control plans—to be encrypted using TLS 1.2 or 1.3. Encryption can be set during installation or created after installation using TLS boot (see Kubernetes documentation) and distribute certificates to nodes. As with all methods, certificates must be distributed between nodes in order to communicate securely. Secret By default, Secrets are stored as unencrypted base64-encoded strings and can be retrieved by anyone with API permissions. Kubernetes Secret maintains sensitive information such as passwords, OAuth tokens, and SSH keys. Storing sensitive information in Secrets provides greater access control than storing passwords or tokens in YAML files, container images, or environment variables. By default, Kubernetes stores Secrets as unencrypted base64-encoded strings that can be retrieved by anyone with API permissions. Access can be restricted by applying an RBAC policy to the secret resource. Secrets can be encrypted by configuring data-at-rest encryption on the API server or by using an external Key Management Service (KMS), which can be provided through a cloud provider. To enable encryption of Secret data at rest using the API server, administrators should modify the kube-apiserver manifest file to execute with the --encryption-provider-config parameter. An example of an encryption-provider-config is shown in Appendix H: Encryption Example. Using a KMS provider prevents the original encryption keys from being stored on local disk. To encrypt a Secret with a KMS provider, the KMS provider should be specified in the encryption-provider-config file, as shown in the KMS configuration example in Appendix I. After applying the encryption-provider-config file, administrators should run the following commands to read and encrypt all Secrets. kubectl get secrets --all-namespaces -o json | kubectl replace -f - Protect sensitive cloud infrastructure Kubernetes is typically deployed on virtual machines in cloud environments. Therefore, administrators should carefully consider the attack surface of the virtual machines running Kubernetes worker nodes. In many cases, pods running on these VMs can access sensitive cloud metadata services on non-routable addresses. These metadata services provide cyber actors with information about cloud infrastructure and perhaps even short-lived credentials for cloud resources. Cyber actors abuse these metadata services for privilege escalation. Kubernetes administrators should prevent pods from accessing cloud metadata services by using network policies or through cloud configuration policies. Because these services vary depending on the cloud provider, administrators should follow the vendor's guidance to harden these access vectors. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"authentication-and-authorization.html":{"url":"authentication-and-authorization.html","title":"Authentication and Authorization","keywords":"","body":"Authentication and authorization Authentication and authorization are the primary mechanisms for restricting access to cluster resources. If a cluster is misconfigured, cyber actors can scan well-known Kubernetes ports and access the cluster's database or make API calls without authentication. User authentication is not a built-in feature of Kubernetes. However, there are several ways for administrators to add authentication to the cluster. Certification The administrator must add an authentication method to the cluster to implement the authentication and authorization mechanism. There are two types of users in a Kubernetes cluster: service accounts and regular user accounts. Service accounts handle API requests on behalf of Pods. Authentication is typically managed automatically by Kubernetes through the ServiceAccount Admission Controller using bearer tokens. Bearer tokens are installed into the Pod at a conventional location and may be used outside the cluster if the token is not secure. Because of this, access to Pod Secrets should be restricted to those who need to view them using Kubernetes RBAC. There is no automatic user authentication method for regular users and administrator accounts. The administrator must add an authentication method to the cluster to implement the authentication and authorization mechanism. Kubernetes assumes that user authentication is managed by a cluster-independent service. Kubernetes documentation lists several methods to implement user authentication, including client certificates, bearer tokens, authentication plugins and others Authentication protocol. At least one method of user authentication should be implemented. When implementing multiple authentication methods, the first module to successfully authenticate a request shortens the evaluation time. Administrators should not use weak methods such as static password files. Weak authentication methods could allow network actors to impersonate legitimate users for authentication. Anonymous requests are requests that are denied by other configured authentication methods and are not associated with any individual user or Pod. In a server with token authentication set up and anonymous requests enabled, requests without a token will be performed as anonymous requests. In Kubernetes 1.6 and newer, anonymous requests are enabled by default. When RBAC is enabled, anonymous requests require explicit authorization from the system:anonymous user or the system:unauthenticated group. Anonymous requests should be disabled by passing the --anonymous-auth=false option to the API server. Enabling anonymous requests may allow network actors to access cluster resources without authentication. Role-based access control RBAC is a method of controlling access to cluster resources based on the roles of individuals within an organization. In Kubernetes 1.6 and newer, RBAC is enabled by default. To use kubectl to check whether RBAC is enabled in your cluster, execute kubectl api-version. If enabled, the API version of rbac.authorization.k8s.io/v1 should be listed. Cloud Kubernetes services may have different ways of checking whether a cluster has RBAC enabled. If RBAC is not enabled, start the API server with the --authorization-mode flag in the following command. kube-apiserver --authorization-mode=RBAC Leaving an authorization mode flag, such as AlwaysAllow, allows all authorization requests, effectively disabling all authorization and limiting the ability to enforce least privilege access. Two types of permissions can be set: Roles and ClusterRoles. Roles sets permissions for a specific namespace, while ClusterRoles sets permissions for all cluster resources regardless of namespace. Roles and ClusterRoles can only be used to add permissions. There are no rejection rules. If a cluster is configured to use RBAC and anonymous access is disabled, the Kubernetes API server will deny permissions that are not explicitly allowed. An example of an RBAC role is shown in Appendix J: the pod-reader RBAC role. A Role or ClusterRole defines a permission but does not bind the permission to a user. RoleBindings and ClusterRoleBindings are used to associate a Roles or ClusterRoles with a user, group or service account. A role binding grants the permissions of a role or cluster role to a user, group, or service account in a defined namespace. ClusterRoles are created independently of the namespace and RoleBinding can then be used to limit the scope of the namespace to individuals. ClusterRoleBindings grant ClusterRoles to a user, group or service account across all cluster resources. Examples of RBAC RoleBinding and ClusterRoleBinding are in [Appendix K: RBAC RoleBinding and ClusterRoleBinding examples] (appendix/k.md). To create or update Roles and ClusterRoles, the user must have the permissions contained in the new role in the same scope, or have access to the Roles or ClusterRoles resource in the rbac.authorization.k8s.io API group Explicit permission to execute the upgrade verb. Once a binding is created, Roles or ClusterRoles are immutable. To change a character, the binding must be deleted. Permissions assigned to users, groups, and service accounts should follow the principle of least privilege and only grant necessary permissions to resources. Users or user groups can be restricted to specific namespaces where the required resources are located. By default, a service account is created for each namespace so that Pods can access the Kubernetes API. You can use RBAC policies to specify the allowed actions for a service account per namespace. Access to the Kubernetes API is restricted by creating RBAC roles or ClusterRoles with the appropriate API request verb and required resources to which the action can be applied. There are tools that can help audit RBAC policies by printing Roles and ClusterRoles for users, groups and service accounts and their associated assignments. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"logging.html":{"url":"logging.html","title":"Log Audit","keywords":"","body":"Log audit Logs record activity in the cluster. Audit logs are necessary not only to ensure that services are running and configured as expected, but also to ensure the security of the system. Systematic audits require consistent and thorough examination of security settings to help identify potential threats. Kubernetes is able to capture audit logs of cluster operations and monitor basic CPU and memory usage information; however, it does not provide in-depth monitoring or alerting services. key point Establish a Pod baseline at creation time to be able to identify anomalous activity. Logging at the host level, application level and cloud (if applicable). Integrate existing network security tools for comprehensive scanning, monitoring, alerting and analysis. Set up local log storage to prevent loss in case of communication failure. log System administrators running applications in Kubernetes should set up an effective logging, monitoring, and alerting system for their environment. Merely logging Kubernetes events is not enough to get the full picture of the actions taking place on the system. Logging should also be done at the host level, application level, and on the cloud (if applicable). Furthermore, these logs can be correlated with any external certification and system logs to provide a complete view of actions taken across the environment for use by security auditors and incident responders. In a Kubernetes environment, administrators should monitor/log the following: API request history Performance Deployment status LF Operating system calls Changes in protocols and permissions Network traffic When a Pod is created or updated, administrators should capture detailed logs of network traffic, response times, requests, resource consumption, and any other relevant metrics to establish a baseline. As detailed in the previous section, anonymous accounts should be disabled, but logging policies should still record actions taken by anonymous accounts to identify anomalous activity. RBAC policy configurations should be audited regularly and whenever there are changes to the organization's system administrators. Doing so ensures that access control adjustments comply with the RBAC policy hardening guidance outlined in the Role-Based Access Control section. Audits should include comparing current logs to baseline measurements of normal activity to identify significant changes in any log metrics and events. System administrators should investigate significant changes—for example, changes in application usage or the installation of malicious programs such as Cipher to determine the root cause. Internal and external traffic logs should be audited to ensure that all expected security restrictions on the connection have been configured correctly and are functioning as expected. Administrators can also use these audits as the system evolves to determine when external access is no longer needed and can be restricted. Logs can be directed to external logging services to ensure they are available to security professionals outside the cluster to identify anomalies as close to real-time as possible and protect logs from deletion if a compromise occurs. If using this method, logs should be encrypted in transit with TLS 1.2 or 1.3 to ensure that cyber actors cannot access the logs in transit and gain valuable information about the environment. Another precaution to take when leveraging an external log server is to configure a log forwarder within Kubernetes with append-only access to the external storage. This helps protect externally stored logs from being deleted or overwritten by in-cluster logs. Kubernetes native audit log configuration The audit function of Kubernetes is disabled by default, so if no audit policy is written, there will be no records. kube-apiserver resides on the Kubernetes control plane and acts as a front-end, handling internal and external requests to the cluster. Every request, whether generated by a user, application, or control plane, generates an audit event at every stage of its execution. When an audit event is registered, kube-apiserver checks the audit policy file and applicable rules. If such a rule exists, the server logs the event at the level defined by the first matching rule. The built-in auditing function of Kubernetes is not enabled by default, so if no audit policy is written, there will be no records. The cluster administrator must write an audit policy YAML file to establish rules and specify the required audit level to log each type of audit event. This audit policy file is then passed to kube-apiserver with the appropriate flags. For a rule to be considered valid, one of four audit levels must be specified: none, Meatadataa, Request, or RequestResponse. Appendix L: Audit Policy shows the contents of an audit policy file that records all events at the RequestResponse level. Appendix M Example of flags for submitting audit policy files to kube-apiserver shows the location of the kube-apiserver configuration file and provides An example of the flags that an audit policy file can be passed to kube-apiserver. Appendix M also provides guidance on how to mount volumes and configure host paths if necessary. kube-apiserver includes configurable logging and webhook backends for audit logs. The logging backend writes specified audit events to a log file, and the webhook backend can be configured to send the file to an external HTTP API. The --audit-log-path and --audit-log-maxage flags set in the example in Appendix M are two examples of ways you can configure a logging backend that writes audit events to a file. The log-path flag is the minimum configuration to enable logging and is the only configuration required for the logging backend. The default format for these log files is JSON, although this can be changed if necessary. Additional configuration options for the logging backend can be found in the Kubernetes documentation. In order to push audit logs to your organization's SIEM platform, the webhook backend can be configured manually via a YAML file submitted to kube-apiserver. The webhook configuration file and how to pass it to kube-apiserver can be viewed in the examples at Appendix N: webhook configuration. An exhaustive list of configuration options on how to set up the webhook backend in kube-apiserver can be found in the Kubernetes documentation. Logging of worker nodes and containers There are many ways to configure logging functionality in a Kubernetes architecture. In the built-in method of log management, the kubelet on each node is responsible for managing logs. It stores and rotates log files locally based on its policies for individual file length, storage time, and storage capacity. These logs are controlled by the kubelet and can be accessed from the command line. The following command prints the logs of the containers in a Pod. kubectl logs [-f] [-p] POD [-c CONTAINER] 如果要对日志进行流式处理，可以使用 -f 标志；如果存在并需要来自容器先前实例的日志，可以使用 -p 标志；如果 Pod 中有多个容器，可以使用 -c 标志来指定一个容器。如果发生错误导致容器、Pod 或节点死亡，Kubernetes 中的本地日志解决方案并没有提供一种方法来保存存储在失败对象中的日志。NSA 和 CISA 建议配置一个远程日志解决方案，以便在一个节点失败时保存日志。 远程记录的选项包括： 远程日志选项 使用的理由 配置实施 在每个节点上运行一个日志代理，将日志推送到后端 赋予节点暴露日志或将日志推送到后端的能力，在发生故障的情况下将其保存在节点之外。 配置一个 Pod 中的独立容器作为日志代理运行，让它访问节点的应用日志文件，并配置它将日志转发到组织的 SIEM。 在每个 Pod 中使用一个 sidecar 容器，将日志推送到一个输出流中 用于将日志推送到独立的输出流。当应用程序容器写入不同格式的多个日志文件时，这可能是一个有用的选项。 为每种日志类型配置 sidecar 容器，并用于将这些日志文件重定向到它们各自的输出流，在那里它们可以被 kubelet 处理。然后，节点级的日志代理可以将这些日志转发给 SIEM 或其他后端。 在每个 Pod 中使用一个日志代理 sidecar，将日志推送到后端 当需要比节点级日志代理所能提供的更多灵活性时。 为每个 Pod 配置，将日志直接推送到后端。这是连接第三方日志代理和后端的常用方法。 从应用程序中直接向后端推送日志 捕获应用程序的日志。Kubernetes 没有内置的机制直接来暴露或推送日志到后端。 各组织将需要在其应用程序中建立这一功能，或附加一个有信誉的第三方工具来实现这一功能。 Sidecar containers run in a Pod along with other containers and can be configured to stream logs to a log file or a logging backend. A sidecar container can also be configured to act as a traffic proxy for another standard functionality container, which is packaged and deployed. To ensure continuity of these logging agents across worker nodes, they are typically run as a DaemonSet. Configuring a DaemonSet for this approach ensures that there is a copy of the logging agent on each node and that any changes made to the logging agent are consistent across the cluster. Seccomp: Audit mode In addition to the node and container logs mentioned above, it can also be very beneficial to log system calls. One way to audit container system calls in Kubernetes is to use the Secure Compute Mode (seccomp) tool. This tool is disabled by default, but can be used to limit the system call capabilities of the container, thereby reducing the kernel's attack surface. Seccomp can also log ongoing calls through the use of audit profiles. A custom seccomp configuration file is used to define which system calls are allowed, and the default action for unspecified calls. To enable a custom seccomp profile in a Pod, Kubernetes administrators can write their seccomp profile JSON file to the /var/lib/kubelet/seccomp/ directory and add seccompProfile to the Pod's securityContext. The custom seccompProfile should also include two fields. Type: Localhost and localhostProfile: myseccomppolicy.json. Logging all system calls can help administrators understand which system calls are required for standard operation, allowing them to further restrict seccomp profiles without losing system functionality. SYSLOG Kubernetes writes kubelet logs and container runtime logs to journald by default, if that service is available. If organizations want to use the syslog tool for systems that are not used by default, or to collect logs from an entire cluster and forward them to a syslog server or other log storage and aggregation platform, they can manually configure this feature. The Syslog protocol defines a log information formatting standard. Syslog messages include a header consisting of a timestamp, hostname, application name, and process ID (PID), and a message written in clear text. Syslog services, such as syslog-ng® and rsyslog, can collect and summarize logs from the entire system in a unified format. Many Linux operating systems default to rsyslog or journald - an event logging daemon that optimizes log storage and outputs logs in syslog format via journalctl. On nodes running some Linux distributions, the syslog tool logs events at the operating system level by default. Containers running these Linux distributions also use syslog to collect logs by default. Logs collected by the syslog tool are stored in the local file system of each applicable node or container, unless a log aggregation platform is configured to collect them. SIEM Platform Security information and event management (SIEM) software collects logs from across an organization's network. SIEM software brings together firewall logs, application logs, and more; parses them out to provide a centralized platform from which analysts can monitor system security. SIEM tools vary in functionality. Generally, these platforms provide log collection, threat detection, and alerting capabilities. Some include machine learning capabilities that can better predict system behavior and help reduce false alarms. Organizations using these platforms in their environments can integrate them with Kubernetes to better monitor and secure their clusters. Open source platforms for managing logs in Kubernetes environments exist as alternatives to SIEM platforms. Containerized environments have many interdependencies between nodes, pods, containers, and services. In these environments, pods and containers are constantly being shut down and restarted on different nodes. This creates additional challenges for traditional SIEMs, which typically use IP addresses to correlate logs. Even next-generation SIEM platforms are not necessarily suitable for complex Kubernetes environments. However, as Kubernetes has become the most widely used container orchestration platform, many organizations developing SIEM tools have developed product variations specifically for Kubernetes environments, providing comprehensive monitoring solutions for these containerized environments. Administrators should understand the capabilities of their platform and ensure their logs adequately capture the environment to support future incident response. Alert Kubernetes does not natively support alerting; however, some monitoring tools with alerting capabilities are compatible with Kubernetes. If a Kubernetes administrator chooses to configure an alerting tool to work in a Kubernetes environment, there are several indicators that the administrator should monitor and configure alerts for. Cases that may trigger an alert include, but are not limited to: Disk space is low on any machine in the environment. The available storage space on the recording volume is decreasing. The external log service is offline. A Pod or application running with root privileges. A request made by an account for a resource to which they do not have permission. An anonymous account that is using or gaining privileges. The IP address of the pod or worker node is listed as the source ID of the pod creation request. Abnormal system calls or failed API calls. The user/admin is behaving unusually (i.e. at an unusual time or from an unusual location), and Significant deviation from the standard operating indicator baseline. Alerting when storage is low can help avoid performance issues and log loss due to limited resources and help identify malicious cryptojacking attempts. Privileged Pod execution cases can be investigated to determine whether an administrator made a mistake, a real use case requires escalating privileges, or a malicious actor deployed a privileged Pod. Suspicious Pod creation source IP addresses may indicate that a malicious network actor has breached the container and attempted to create a malicious Pod. Integrating Kubernetes with an enterprise's existing SIEM platforms, especially those with machine learning/big data capabilities, can help identify violations in audit logs and reduce false alarms. If such a tool is configured to work with Kubernetes, it should be configured to trigger alerts for these conditions and any other conditions applicable to the use case. Systems capable of taking automated action when a suspected intrusion occurs can potentially be configured to take steps to mitigate damage as administrators respond to alerts. In cases where a Pod IP is listed as the source ID of a Pod creation request, one mitigation that can be implemented is to automatically evict the Pod to keep the application available but temporarily halt any damage to the cluster. Doing so will allow a clean pod version to be rescheduled onto a node. Investigators can then examine the logs to determine whether a breach occurred and, if so, investigate how the malicious actor carried out the potential threat so that a patch can be deployed. Service Grid A service mesh is a platform that simplifies microservice communication within an application by allowing these communication logic to be encoded into the service mesh, rather than within each microservice. Encoding this communication logic into individual microservices is difficult to scale, difficult to debug when failures occur, and difficult to ensure security. Using a service mesh simplifies a developer's job. A service mesh can: Redirect traffic when a service is interrupted. Collect performance metrics to optimize communications. Allows management of service-to-service communication encryption. Collect logs of inter-service communications. Collect logs from each service. Help developers diagnose problems and failures in microservices or communication mechanisms. Service meshes can also help migrate services to hybrid or multi-cloud environments. While service meshes are not required, they are an option that is highly suitable for Kubernetes environments. Managed Kubernetes services typically include their own service mesh. However, several other platforms are available and can be highly customized if desired. Some include a certificate authority that generates and rotates certificates, allowing secure TLS authentication between services. Administrators should consider using a service mesh to harden the security of their Kubernetes clusters. Figure 5: Cluster leverages service mesh to combine logs with network security Fault Tolerance A fault tolerance strategy should be developed to ensure the availability of the log service. These strategies can vary based on the specific Kubernetes use case. One strategy that can be implemented is to allow new logs to overwrite the oldest log files if it is absolutely necessary to exceed storage capacity. If logs are sent to an external service, a mechanism should be established to store the logs locally in the event of a communication interruption or external service failure. Once communication with the external service is restored, a strategy should be developed to push locally stored logs to the external server. tool Kubernetes does not include extensive auditing capabilities. However, the system is built to be extensible, allowing users the freedom to develop their own custom solutions or select existing add-ons that suit their needs. One of the most common solutions is to add an additional audit backend service that can consume the information logged by Kubernetes and perform additional functionality for users, such as extended search parameters, data mapping capabilities, and alerting capabilities. Enterprises already using SIEM platforms can integrate Kubernetes with these existing capabilities. Open source monitoring tools, such as Cloud Native Computing Foundation's Prometheus®, Grafana Labs' Grafana®, and Elasticsearch's Elastic Stack (ELK)® - can be used to monitor events, run threat analysis, manage alerts, and collect resource isolation parameters and historical usage status and network statistics of running containers. When auditing access control and permission configurations, scanning tools can be useful by assisting in identifying risky permission configurations in RBAC. The NSA and CISA encourage organizations using intrusion detection systems (IDS) in existing environments to consider integrating the service into their Kubernetes environments as well. This integration will enable enterprises to monitor and potentially kill containers that show signs of unusual behavior, allowing containers to be restarted from an initially clean image. Many cloud service providers also offer container monitoring services for those who want a more managed and scalable solution. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"upgrading-and-application-security-practices.html":{"url":"upgrading-and-application-security-practices.html","title":"Upgrading and application security practices","keywords":"","body":"Upgrade and apply security practices Following the hardening guidelines outlined in this document is one step in ensuring the security of applications running on Kubernetes orchestration containers. However, security is an ongoing process and it is critical to keep up with patches, updates, and upgrades. The specific software components will vary based on individual configurations, but every piece of the entire system should be kept as secure as possible. This includes updates to: Kubernetes, the hypervisor, virtualization software, plug-ins, the operating system the environment runs on, the applications running on the servers, and any other software hosted in the Kubernetes environment. The Center for Internet Security (CIS) publishes benchmarks for securing software. Administrators should adhere to the CIS baseline for Kubernetes and any other related system components. Administrators should regularly check to ensure that the security of their systems complies with the current consensus among security experts on best practices. Regular vulnerability scanning and penetration testing of various system components should be performed to proactively look for insecure configurations and zero-day vulnerabilities. Any findings should be promptly remediated before potential cyber actors discover and exploit them. As updates are deployed, administrators should also keep up with removing any old components from the environment that are no longer needed. Using a managed Kubernetes service can help automate upgrades and patching of Kubernetes, operating systems, and network protocols. However, administrators still have to patch and upgrade their containerized applications. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:47 "},"appendix/a.html":{"url":"appendix/a.html","title":"Appendix A: Example Dockerfile for non-root applications","keywords":"","body":"Appendix A: Dockerfile example for non-root application The following example is a Dockerfile that runs an application as a non-root user and non-group member. FROM ubuntu:latest # Upgrade and install the make tool RUN apt update && apt install -y make # Copy the source code from a folder called code and build the application using the make tool. COPY ./code RUN make /code # Create a new user (user1) and a new group (group1); then switch to the context of the user. RUN useradd user1 && groupadd group1 USER user1:group1 #Set the default entry for the container CMD /code/app Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/b.html":{"url":"appendix/b.html","title":"Appendix B: Deployment template example for read-only file system","keywords":"","body":"Appendix B: Deployment template example for read-only file system Below is an example of a Kubernetes deployment template using a read-only root file system. apiVersion: apps/v1 Kind: Deployment metadata: labels: app: web name: web spec: selector: matchLabels: app: web template: metadata: labels: app: web name: web spec: containers: - command: [\"sleep\"] args: [\"999\"] image:ubuntu:latest name:web securityContext: readOnlyRootFilesystem: true #Make the container's file system read-only volumeMounts: - mountPath: /writeable/location/here #Create a writable volume name:volName volumes: - emptyDir: {} name:volName Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/c.html":{"url":"appendix/c.html","title":"Appendix C: Pod Security Policy Example","keywords":"","body":"Appendix C: Pod security policy example: PS: PSP are deprecated and removed since version v1.25 Below is an example of a Kubernetes Pod security policy that enforces strong security requirements for containers running in the cluster. This example is based on official Kubernetes documentation. Administrators are encouraged to modify this policy to meet their organization's requirements. apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' spec: privileged: false # Need to prevent upgrade to root allowPrivilegeEscalation: false requiredDropCapabilities: - ALL volumes: -'configMap' - 'emptyDir' - 'projected' - 'secret' -'downwardAPI' - 'persistentVolumeClaim' # Assume persistentVolumes set by the administrator are safe hostNetwork: false hostIPC: false hostPID: false runAsUser: rule: 'MustRunAsNonRoot' #Require containers to run seLinux without root rule: 'RunAsAny' # Assume the node is using AppArmor instead of SELinux supplementalGroups: rule: 'MustRunAs' ranges: # Disable adding to root group - min: 1 max: 65535 runAsGroup: rule: 'MustRunAs' ranges: # Disable adding to root group - min: 1 max: 65535 fsGroup: rule: 'MustRunAs' ranges: # Disable adding to root group - min: 1 max: 65535 readOnlyRootFilesystem: true Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/d.html":{"url":"appendix/d.html","title":"Appendix D: Namespace Example","keywords":"","body":"Appendix D: Namespace Example The following example creates a Kubernetes namespace for each team or user group using the kubectl command or YAML file. Any name prefixed with kube should be avoided as it may conflict with the namespace reserved by the Kubernetes system. Kubectl command to create a namespace. kubectl create namespace To create a namespace using a YAML file, create a new file named my-namespace.yaml with the following content: apiVersion: v1 kind: Namespace metadata: name: To apply the namespace, use: kubectl create –f ./my-namespace.yaml To create a new Pod in an existing namespace, switch to the desired namespace: kubectl config use-context To apply a new Deployment, use: kubectl apply -f deployment.yaml Alternatively, you can add the namespace to the kubectl command using: kubectl apply -f deployment.yaml --namespace= Or specify namespace: under metadata in the YAML declaration. Once created, resources cannot be moved between namespaces. The resource must be deleted and created in a new namespace. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/e.html":{"url":"appendix/e.html","title":"Appendix E: Network Policy Example","keywords":"","body":"Appendix E: Network Policy Example Network policies vary depending on the network plug-in used. The following is an example of a network policy. Refer to Kubernetes documentation to limit access to the nginx service to Pods with labeled access. superior. apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: example-access-nginx namespace: prod #This can be any namespace, or omitted if no namespace is used. spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: access: \"true\" The new NetworkPolicy can be applied in the following ways: kubectl apply -f policy.yaml A default policy that denies all entries: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-ingress spec: podSelector: {} policyType: -Ingress A default policy that denies all exports: apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress spec: podSelector: {} policyType: - Egress Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/f.html":{"url":"appendix/f.html","title":"Appendix F: LimitRange Example","keywords":"","body":"Appendix F: LimitRange Example In Kubernetes 1.10 and newer, LimitRange support is enabled by default. The following YAML file specifies a LimitRange for each container, which has a default request and limit, as well as a minimum and maximum request. apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits -default: cpu: 1 defaultRequest: CPU: 0.5 max: cpu: 2 min: CPU 0.5 type: Container LimitRange can be applied to namespaces, using: kubectl apply -f .yaml --namespace= After applying this LimitRange configuration example, all containers created in the namespace will be assigned the default CPU requests and limits if not specified. The CPU requests of all containers in the namespace must be greater than or equal to the minimum value and less than or equal to the maximum CPU value, otherwise the container will not be instantiated. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/g.html":{"url":"appendix/g.html","title":"Appendix G: ResourceQuota Example","keywords":"","body":"Appendix G: ResourceQuota Example Create a ResourceQuota object to limit overall resource usage within a namespace by applying a YAML file to the namespace or specifying requirements in the Pod's configuration file. The following example is an example of a namespace configuration file based on Kubernetes official documentation: apiVersion: v1 kind: ResourceQuota metadata: name: example-cpu-mem-resourcequota spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi This ResourceQuota can be applied like this: kubectl apply -f example-cpu-mem-resourcequota.yaml -- namespace= This ResourceQuota imposes the following restrictions on the selected namespace: Each container must have a memory request, memory limit, CPU request, and CPU limit. Total memory requests across all containers should not exceed 1 GiB The total memory limit for all containers should not exceed 2 GiB Total CPU requests for all containers should not exceed 1 CPU The total CPU limit for all containers should not exceed 2 CPUs Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/h.html":{"url":"appendix/h.html","title":"Appendix H: Encryption Example","keywords":"","body":"Appendix H: Encryption Example To encrypt secret data at rest, the encryption configuration file below provides an example to specify the required encryption type and encryption key. Storing the encryption key in an encrypted file only slightly improves security. The Secret will be encrypted, but the key will be accessed in the EncryptionConfiguration file. This example is based on Kubernetes official documentation. apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: - identity: {} To use this encryption file for encryption at rest, set the --encryption-provider-config flag when restarting the API server and indicate the location of the configuration file. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/i.html":{"url":"appendix/i.html","title":"Appendix I: KMS configuration example","keywords":"","body":"Appendix I: KMS configuration example To encrypt a Secret with a Key Management Service (KMS) provider plug-in, you can use the following example encryption configuration YAML file to set properties for the provider. This example is based on Kubernetes official documentation. apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} To configure the API server to use a KMS provider, set the --encryption-provider-config flag along with the location of the configuration file and restart the API server. To switch from a local encryption provider to KMS, add the KMS provider section in the EncryptionConfiguration file above the current encryption method as shown below. apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - aescbc: keys: - name: key1 secret: Restart the API server and run the following command to re-encrypt all secrets with the KMS provider. kubectl get secrets --all-namespaces -o json | kubectl replace -f - Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/j.html":{"url":"appendix/j.html","title":"Appendix J: pod-reader RBAC role","keywords":"","body":"Appendix J: pod-reader RBAC role To create a pod-reader role, create a YAML file with the following content: apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: your-namespace-name name: pod-reader rules: - apiGroups: [\"\"] # \"\" represents the core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] Application role: kubectl apply --f role.yaml To create a global pod-reader ClusterRole: apiVersion: rbac.authorization.k8s.io/v1 kind:ClusterRole metadata: default # \"namespace\" is omitted because ClusterRoles is not bound to a namespace name: global-pod-reader rules: - apiGroups: [\"\"] # \"\" represents the core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] Application role: kubectl apply --f clusterrole.yaml Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/k.html":{"url":"appendix/k.html","title":"Appendix K: RBAC RoleBinding and ClusterRoleBinding examples","keywords":"","body":"Appendix K: RBAC RoleBinding and ClusterRoleBinding examples To create a RoleBinding, create a YAML file with the following content: apiVersion: rbac.authorization.k8s.io/v1 # This role binding allows \"jane\" to read the Pod namespace of \"your-namespace-name\" # You need to already have a role named \"pod-reader\" in this namespace. kind: RoleBinding metadata: name: read-pods namespace: your-namespace-name subjects: # You can specify more than one \"subject\" - kind: User name: jane # \"name\" is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies binding to a Role/ClusterRole kind: Role # Must be Role or ClusterRole name: pod-reader # This must match the name of the Role or ClusterRole you want to bind apiGroup: rbac.authorization.k8s.io Apply RoleBinding: kubectl apply --f rolebinding.yaml To create a ClusterRoleBinding, create a YAML file with the following content: apiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows anyone in the \"manager\" group to read Pod information in any namespace. kind: ClusterRoleBinding metadata: name: global-pod-reader subjects: # You can specify more than one \"subject\" - kind: Group name: manager # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: # \"roleRef\" specifies binding to a Role/ClusterRole kind: ClusterRole # Must be Role or ClusterRole name: global-pod-reader # This must match the name of the Role or ClusterRole you want to bind apiGroup: rbac.authorization.k8s.io Apply RoleBinding: kubectl apply --f clusterrolebinding.yaml Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/l.html":{"url":"appendix/l.html","title":"Appendix L: Audit Strategy","keywords":"","body":"Appendix L: Audit Strategy Here is an audit policy that logs all audit events at the highest level: apiVersion: audit.k8s.io/v1 Kind: Policy rules: - level: RequestResponse # This audit policy records all audit events at the RequestResponse level This audit strategy logs all events at the highest level. If an organization has the resources available to store, parse, and examine large volumes of logs, logging all events at the highest level is a good way to ensure that when an event occurs, all necessary contextual information is present in the logs. If resource consumption and availability are an issue, then additional logging rules can be established to reduce logging levels for non-critical components and general unprivileged operations, as long as the auditing requirements of the system are met. Examples of how to set up these rules can be found in the Kubernetes official documentation. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/m.html":{"url":"appendix/m.html","title":"Appendix M: Example of flags for submitting audit policy files to kube-apiserver","keywords":"","body":"Appendix M: Example of flags for submitting audit policy files to kube-apiserver In the control plane, open the kube-apiserver.yaml file with a text editor. Editing the kube-apiserver configuration requires administrator privileges. sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml Add the following text to the kube-apiserver.yaml file: --audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 The audit-policy-file flag should be set to the path to the audit policy, and the audit-log-path flag should be set to a safe location where the required audit logs are written. There are some other flags, such as the audit-log-maxage flag shown here, which specifies the maximum number of days that logs should be kept, and there are also flags that specify the maximum number of audit log files to retain, the maximum Log file size in megabytes, etc. The only necessary flags to enable logging are the audit-policy-file and audit-log-path flags. Other flags can be used to configure logging to comply with your organization's policies. If the user's kube-apiserver is running as a Pod, then it is necessary to mount the volume and configure the hostPath of the policy and log file location to preserve audit records. This can be done by adding the following section to the kube-apiserver.yaml file as pointed out in the Kubernetes documentation: volumeMounts: - mountPath: /etc/kubernetes/audit-policy.yaml name: audit readOnly: true - mountPath: /var/log/audit.log name: audit-log readOnly: false volumes: - hostPath: path: /etc/kubernetes/audit-policy.yaml type: File name: audit - hostPath: path: /var/log/audit.log type: FileOrCreate name: audit-log Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "},"appendix/n.html":{"url":"appendix/n.html","title":"Appendix N: webhook configuration","keywords":"","body":"Appendix N: webhook configuration YAML file example: apiVersion: v1 Kind: Config preferences: {} clusters: - name: example-cluster cluster: server: http://127.0.0.1:8080 #web endpoint address for the log files to be sent to name: audit-webhook-service users: - name: example-users user: username: example-user password: example-password contexts: - name: example-context context: cluster: example-cluster user: example-user current-context: example-context #source: https://dev.bitolog.com/implement-audits-webhook/ Audit events sent by webhooks are sent as HTTP POST requests with JSON audit events included in the request body. The specified address should point to an endpoint capable of accepting and parsing these audit events, whether it is a third-party service or an in-house configured endpoint. Example flags for submitting a webhook configuration file to kube-apiserver: Edit the kube-apiserver.yaml file in the control plane sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml Add the following text in the kube-apiserver.yaml file --audit-webhook-config-file=/etc/kubernetes/policies/webhook-policy.yaml --audit-webhook-initial-backoff=5 --audit-webhook-mode=batch --audit-webhook-batch-buffer-size=5 The audit-webhook-initial-backoff flag determines how long to wait after an initial failed request before retrying. The available webhook modes are batch, block and blocking-stric. When using batch mode, it is possible to configure maximum wait time, buffer size, etc. The official Kubernetes documentation contains more details on other configuration options audit and kube-apiserver. Copyright © 2023 - 2024 | Saif(eddine) Rajhi all right reserved. Updated at 2024-02-22 21:23:46 "}}